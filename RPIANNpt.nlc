//RPIANNpt Specification (prompts):

---
v1a (Codex IDE GPT5Codex prompt);

Please upgrade RPIANNpt_RPIANNmodel to implement this specification;

I have an input embedding x and a prediction embedding Z, all encoded in a single recursive MLP layer; the layer decides what best next action the network should take to iteratively improve the prediction embedding Z (with respect to the target output embedding Z_target). For simplicity, assume a large random projection from class targets to prediction/target output embedding (Z/Z_target), and a large random projection from input to input embedding x.

---
trainLocal (Codex IDE GPT5Codex prompt):

OK please perform this upgrade. When trainLocal=True, then only perform backprop between individual recursive iterations (and never in the input/output embedding layers). If trainLocal=False, then perform full backprop through the entire recursive network.

---
useCNNlayers (Codex IDE GPT5Codex prompt):

input_projection should be changed to an untrained CNN layer when useImageDataset=True (that still produces the correct embedding size

---
useRecursiveLayers (Codex IDE GPT5Codex prompt):

Please implement option useRecursiveLayers=False (while leaving the current implementation for useRecursiveLayers=True unchanged). If useRecursiveLayers=False then;
a) class RPIANNmodel creates independent nonrecursive_layer object (ie parameters) for each layer - instead of a single recursive_layer object.
b) class RPIANNmodel: iterate_prediction() and _train_recursive_locally() functions are updated to execute using these nonrecursive_layer objects (instead of the recursive_layer object).

---
targetProjectionActivationFunction=False (Codex IDE GPT5Codex prompt):

Please implement option targetProjectionActivationFunction=False - the final hidden layer in the network does not have a relu applied (enabling a signed comparison with the signed output of the target projection).

---
subLayerFirstNotTrained (Codex IDE GPT5Codex prompt):

Hi, please implement option subLayerFirstNotTrained=True (only when numberOfSublayers>1)

---
subLayerFirstMixXembedZStreamsSeparately (Codex IDE GPT5Codex prompt):

Please implement option subLayerFirstMixXembedZStreamsSeparately=True (for case numberOfSublayers >1 and layersFeedConcatInput=True only); it feeds x_embed and Z input through separate weights in the first sublayer (then concats their output).

---
subLayerFirstSparse (Codex IDE GPT5Codex prompt):

Please implement option subLayerFirstSparse=True (for case numberOfSublayers >1 and subLayerFirstNotTrained=True only). This initialises the all first sublayer weights to be sparsely connected rather than densly connected, while providing a parameter to set the level of this sparsity.

---
useRPICNN (Codex IDE GPT5Codex prompt):

Please implement option useRPICNN=True (for case useImageDataset=True and useCNNlayers=True only); 
- useRPICNN creates a standard CNN network but with conv and maxpool kernels with stride=1 (the cnn "pixel" space does not shrink or expand each layer), and applies an equal number of kernels per layer.
- similar to the core RPIANN algorithm; when layersFeedConcatInput=True, each RPICNN kernel takes input from both the first x embedding layer and the output of the previous embedding layer.
- similar to the core RPIANN algorithm; when useRecursiveLayers=True, each layer reuses its RPICNN kernels (they are not initialised separately for each layer).

---
useRPICNN (Codex IDE GPT5Codex prompt):

There is no theoretical reason why the num channels for x_embed and Z are identical for an RPICNN architecture (for initialiseZero=True).
1. Please upgrade the useRPICNN code to support different number of channels for x_embed. 
2. please replace _build_rpi_cnn_projection with _build_image_projection.
3. please upgrade _build_image_projection to support configurable stride values (ie stride 1 or stride 2).
4. please add bool option useRPIANNimageProjection=False, and set it to False by default. In this case x_embed will have 3 channels (corresponding to the CIFAR image channels).

Please proceed with implementation, but also inform me of any algorithmic issues you encounter when thinking about the new implementation.

---
RPICNNuniqueWeightsPerPixel (Codex IDE GPT5Codex prompt):

Please implement option RPICNNuniqueWeightsPerPixel=True (for case useRPICNN=True). Each pixel of the RPICNN action layer has its own unique CNN kernel weights.

---
targetProjectionExemplarImage (Codex IDE GPT5Codex prompt):

Please implement option targetProjectionExemplarImage=True (for case useImageDataset=True); the embedding layer target is created by selecting the first image in the train dataset of the particular class target.
- If useCNNlayers=True, this exemplar image is fed through an untrained CNN (using the same code as the image input projection) to produce the embedding layer target (for the useClassificationLayerLoss=False training regime). 
- If useCNNlayers=False, this exemplar image is fed through a standard linear target projection matrix of the appropriate dimensions to produce the embedding layer target (for the useClassificationLayerLoss=False training regime).

To achieve this, please update encode_targets() accordingly for case targetProjectionExemplarImage=True;
1. generate the exemplar image from the class target
2. self.target_projection will either be an image projection (useCNNlayers=True) or a linear projection (useCNNlayers=False).

---
targetProjectionSparse (Codex IDE GPT5Codex prompt):

add option targetProjectionSparse=True - modify target_projection to generate a sparse instead of dense target embedding (for case useCNNtargetProjection=False), while providing a parameter to set the level of this sparsity.

---
targetProjectionUniquePerLayer (Codex IDE GPT5Codex prompt):

Please create option targetProjectionUniquePerLayer = True - create a separate target embedding for each RPI layer.

---
independent useRPICNN and useRPIANN (Codex IDE GPT5Codex prompt):

Please upgrade useRPICNN=True to create independent CNNActionLayers (numberOfConvlayers) and ActionLayers (numberOfFFLayers), instead of just creating just CNNActionLayers. useRPICNN=False still creates the same number of ActionLayers (numberOfLayers).

---
trainFinalIterationOnly (Codex IDE GPT5Codex prompt):

Please implement option trainFinalIterationOnly=True; where only the final hidden layer (or iteration if useRecursiveLayers=True) is trained.

---
debugPrintConcatWeights (Codex IDE GPT5Codex prompt):

... please add option debugPrintConcatWeights=True to print (using useLovelyTensors=True);
a) the Z weights of layersFeedConcatInput, and;
b) the xEmbed weights of layersFeedConcatInput for each batch.

Implementation: When debugPrintConcatWeights=True, complete my pseudo code in the #TODO section of _ActionLayerBase:forward().

---
trainClassificationLayer (Codex IDE GPT-5.1-Codex prompt):

Please implement option trainClassificationLayer=True to enable training of classification layer when useClassificationLayerLoss=True (classification layer weights are modified based on backprop gradients).

---
inputProjectionAutoencoder/targetProjectionAutoencoder (Codex IDE GPT-5.1-Codex prompt):

Please implement options inputProjectionAutoencoder=True and targetProjectionAutoencoder=True; when these options are enabled the input and/or target projections are trained using an autoencoder algorithm;

inputProjectionAutoencoder: input->Z->input (separate matrices for each direction; forward->reverse) 
targetProjectionAutoencoder: output->Z->output (separate matrices for each direction; forward->reverse)

Please also implement options inputProjectionAutoencoderIndependent=True and targetProjectionAutoencoderIndependent=True; when these options are also enabled, each direction of the autoencoders is trained independently by holding the other direction weights constant while it is being trained. Each batch processed by RPIANN will therefore apply 2 autoencoder subtraining rounds, "forward"/"reverse" (where this naming scheme is relative to the individual autoencoders not the entire network);

a) "forward" subtraining round will train the autoencoder forward matrices (of the input and/or target projections);
- The autoencoder reverse layer is executed to obtain the activations of Z.
- the autoencoder forward layer is then executed to obtain the predicted activations of Z_hat.
- single layer backprop is then applied to train the forward layer weights (using the error of Z_hat/Z).

b) "reverse" subtraining round will train the autoencoder reverse matrices (of the input and/or target projections);
- The autoencoder forward layer is executed to obtain the activations of Z.
- the autoencoder reverse layer is then executed to obtain the x_hat or y_hat values.
- single layer backprop is then applied to train the reverse layer weights (using the error of x_hat/x or y_hat/y).

When inputProjectionAutoencoderIndependent=False and targetProjectionAutoencoderIndependent=False, each autoencoder is trained using 2 layer backprop.

	Thank you for performing this upgrade. Before I test it myself I want you to verify the requirements;

	a) when inputProjectionAutoencoder=False and targetProjectionAutoencoder=False, RPIANN behaves exactly like it did before the upgrade; I.e. 
	i) if trainClassificationLayer=False, both the input and target projections will be untrained. 
	ii) if trainClassificationLayer=True it will still train the target layer.

	b) when inputProjectionAutoencoder=True and targetProjectionAutoencoder=True and inputProjectionAutoencoderIndependent=False and targetProjectionAutoencoderIndependent=False, the input and target autoencoders (both forward and reverse direction matrices) are trained using 2 layer backprop (standard autoencoder algorithm).

	c) when inputProjectionAutoencoder=True and targetProjectionAutoencoder=True and inputProjectionAutoencoderIndependent=True and targetProjectionAutoencoderIndependent=True, the input and target autoencoder forward and reverse direction matrices are trained separately using the single layer backprop autoencoder training method specified in the requirements.

	If there have been any changes to the expected implementation, please notify me of these changes.

---
useInputProjection (Codex IDE GPT-5.1-Codex prompt):

Please add bool option useInputProjection. If useInputProjection=True and useImageDataset=True and useCNNinputProjection=False, then always create and execute a linear input projection (currently none is being used). If useInputProjection=False, then never create and execute a linear input projection (even when useImageDataset=False).

Make option inputProjectionAutoencoder=True dependent on useInputProjection=True and useCNNinputProjection=False.

Make option targetProjectionAutoencoder=True explicitly dependent on useCNNtargetProjection=False.

---
support x_embed.shape != Z.shape (Codex IDE GPT-5.1-Codex prompt):

Please upgrade all cases (especially case use_linear_input_projection=True) to support x_embed.shape != Z.shape, and remove the mlp_input_adapter code completely (as this is no longer requried). If you identify a problem with this upgrade please let me know first.

---
projectionAutoencoderWarmupEpochs + projectionAutoencoderDenoisingStd + projectionAutoencoderPretrainEpochs (Codex Cloud GPT-5.1-Codex prompt):

...

	## Projection autoencoder accuracy considerations

	Enabling `inputProjectionAutoencoder` or `targetProjectionAutoencoder` adds a reconstruction objective on the projection layers before every training step. The model first runs the autoencoders and updates the projection weights, then uses those updated weights to encode the batch for the main loss.【F:RPIANNpt/RPIANNpt_RPIANNmodel.py†L319-L337】 Because the autoencoder loss is mean-squared reconstruction of the raw inputs or one-hot targets【F:RPIANNpt/RPIANNpt_RPIANNmodelAutoencoder.py†L144-L155】【F:RPIANNpt/RPIANNpt_RPIANNmodelAutoencoder.py†L218-L234】, it regularizes the projections toward identity-like mappings rather than the task-specific embeddings that maximize downstream accuracy. When projection weights would otherwise stay fixed (for example, the linear input projection is frozen when the autoencoder is disabled)【F:RPIANNpt/RPIANNpt_RPIANNmodel.py†L126-L148】, the extra reconstruction updates can introduce non-stationary inputs for the rest of the network and compete with the main optimization objective. This mismatch often leads to slightly lower classification accuracy despite the better reconstruction of the original signals.

	To capture the reconstruction benefits without letting the auxiliary objective drift the projections away from the task, you can enable a short warmup and stop autoencoder updates afterward by setting `projectionAutoencoderWarmupEpochs` to a small value (for example, 3–5 epochs). This pretrains the projections for faithful reconstruction and then lets the classification loss take over, which tends to improve downstream accuracy compared with leaving the autoencoders active for every epoch.

	If the autoencoders still collapse toward identity-like projections, turn them into denoising autoencoders by setting `projectionAutoencoderDenoisingStd` to a small nonzero standard deviation (for example, 0.05–0.1). During autoencoder training this injects Gaussian noise into the inputs/one-hot targets while asking the reverse projections to reconstruct the clean originals【F:RPIANNpt/RPIANNpt_RPIANN_globalDefs.py†L96-L107】【F:RPIANNpt/RPIANNpt_RPIANNmodelAutoencoder.py†L73-L78】【F:RPIANNpt/RPIANNpt_RPIANNmodelAutoencoder.py†L144-L155】【F:RPIANNpt/RPIANNpt_RPIANNmodelAutoencoder.py†L218-L234】. The projections must therefore encode denoised structure instead of copying inputs directly, which typically yields more stable embeddings for the downstream task.

	To decouple the projection reconstruction objective from the main task even further, you can pretrain the autoencoders before normal RPIANN updates by setting `projectionAutoencoderPretrainEpochs` to a positive integer. This runs dedicated epochs that only optimize the projection autoencoders (using their own optimizers) before any task loss is applied, reporting the input and target reconstruction losses as it goes.【F:RPIANNpt/ANNpt_main.py†L131-L163】【F:RPIANNpt/RPIANNpt_RPIANNmodel.py†L268-L279】【F:RPIANNpt/RPIANNpt_RPIANNmodelAutoencoder.py†L80-L109】【F:RPIANNpt/RPIANNpt_RPIANN_globalDefs.py†L96-L107】 After the pretraining window, you can either keep a short warmup (`projectionAutoencoderWarmupEpochs`) or disable the autoencoders entirely, which often yields higher accuracy than interleaving reconstruction and task updates every epoch because the projections start from a well-shaped embedding space and are no longer pulled toward identity during supervised training.

