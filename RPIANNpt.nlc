//RPIANNpt Specification (prompts):

---
v1a (Codex IDE GPT5Codex prompt);

Please upgrade RPIANNpt_RPIANNmodel to implement this specification;

I have an input embedding x and a prediction embedding y_hat, all encoded in a single recursive MLP layer; the layer decides what best next action the network should take to iteratively improve the prediction embedding y_hat (with respect to the target output embedding y). For simplicity, assume a large random projection from class targets to prediction/target output embedding y_hat/y, and a large random projection from input to input embedding x.

---
trainLocal (Codex IDE GPT5Codex prompt):

OK please perform this upgrade. When trainLocal=True, then only perform backprop between individual recursive iterations (and never in the input/output embedding layers). If trainLocal=False, then perform full backprop through the entire recursive network.

---
useCNNlayers (Codex IDE GPT5Codex prompt):

input_projection should be changed to an untrained CNN layer when useImageDataset=True (that still produces the correct embedding size

---
useRecursiveLayers (Codex IDE GPT5Codex prompt):

Please implement option useRecursiveLayers=False (while leaving the current implementation for useRecursiveLayers=True unchanged). If useRecursiveLayers=False then;
a) class RPIANNmodel creates independent nonrecursive_layer object (ie parameters) for each layer - instead of a single recursive_layer object.
b) class RPIANNmodel: iterate_prediction() and _train_recursive_locally() functions are updated to execute using these nonrecursive_layer objects (instead of the recursive_layer object).

---
targetProjectionActivationFunction=False (Codex IDE GPT5Codex prompt):

Please implement option targetProjectionActivationFunction=False - the final hidden layer in the network does not have a relu applied (enabling a signed comparison with the signed output of the target projection).

---
subLayerFirstNotTrained (Codex IDE GPT5Codex prompt):

Hi, please implement option subLayerFirstNotTrained=True (only when numberOfSublayers>1)

---
subLayerFirstMixXembedYhatStreamsSeparately (Codex IDE GPT5Codex prompt):

Please implement option subLayerFirstMixXembedYhatStreamsSeparately=True (for case numberOfSublayers >1 and layersFeedConcatInput=True only); it feeds x_embed and y_hat input through separate weights in the first sublayer (then concats their output).

---
subLayerFirstSparse (Codex IDE GPT5Codex prompt):

Please implement option subLayerFirstSparse=True (for case numberOfSublayers >1 and subLayerFirstNotTrained=True only). This initialises the all first sublayer weights to be sparsely connected rather than densly connected, while providing a parameter to set the level of this sparsity.

---
useRPICNN (Codex IDE GPT5Codex prompt):

Please implement option useRPICNN=True (for case useImageDataset=True and useCNNlayers=True only); 
- useRPICNN creates a standard CNN network but with conv and maxpool kernels with stride=1 (the cnn "pixel" space does not shrink or expand each layer), and applies an equal number of kernels per layer.
- similar to the core RPIANN algorithm; when layersFeedConcatInput=True, each RPICNN kernel takes input from both the first x embedding layer and the output of the previous embedding layer.
- similar to the core RPIANN algorithm; when useRecursiveLayers=True, each layer reuses its RPICNN kernels (they are not initialised separately for each layer).

---
useRPICNN (Codex IDE GPT5Codex prompt):

There is no theoretical reason why the num channels for x_embed and y_hat are identical for an RPICNN architecture (for initialiseYhatZero=True).
1. Please upgrade the useRPICNN code to support different number of channels for x_embed. 
2. please replace _build_rpi_cnn_projection with _build_image_projection.
3. please upgrade _build_image_projection to support configurable stride values (ie stride 1 or stride 2).
4. please add bool option useRPIANNimageProjection=False, and set it to False by default. In this case x_embed will have 3 channels (corresponding to the CIFAR image channels).

Please proceed with implementation, but also inform me of any algorithmic issues you encounter when thinking about the new implementation.

---
RPICNNuniqueWeightsPerPixel (Codex IDE GPT5Codex prompt):

Please implement option RPICNNuniqueWeightsPerPixel=True (for case useRPICNN=True). Each pixel of the RPICNN action layer has its own unique CNN kernel weights.

---
targetProjectionExemplarImage (Codex IDE GPT5Codex prompt):

Please implement option targetProjectionExemplarImage=True (for case useImageDataset=True); the embedding layer target is created by selecting the first image in the train dataset of the particular class target.
- If useCNNlayers=True, this exemplar image is fed through an untrained CNN (using the same code as the image input projection) to produce the embedding layer target (for the useClassificationLayerLoss=False training regime). 
- If useCNNlayers=False, this exemplar image is fed through a standard linear target projection matrix of the appropriate dimensions to produce the embedding layer target (for the useClassificationLayerLoss=False training regime).

To achieve this, please update encode_targets() accordingly for case targetProjectionExemplarImage=True;
1. generate the exemplar image from the class target
2. self.target_projection will either be an image projection (useCNNlayers=True) or a linear projection (useCNNlayers=False).

---
targetProjectionSparse (Codex IDE GPT5Codex prompt):

add option targetProjectionSparse=True - modify target_projection to generate a sparse instead of dense target embedding (for case useCNNtargetProjection=False), while providing a parameter to set the level of this sparsity.

---
targetProjectionUniquePerLayer (Codex IDE GPT5Codex prompt):

Please create option targetProjectionUniquePerLayer = True - create a separate target embedding for each RPI layer.

---
independent useRPICNN and useRPIANN (Codex IDE GPT5Codex prompt):

Please upgrade useRPICNN=True to create independent CNNActionLayers (numberOfConvlayers) and ActionLayers (numberOfFFLayers), instead of just creating just CNNActionLayers. useRPICNN=False still creates the same number of ActionLayers (numberOfLayers).

---
trainFinalIterationOnly (Codex IDE GPT5Codex prompt):

Please implement option trainFinalIterationOnly=True; where only the final hidden layer (or iteration if useRecursiveLayers=True) is trained.

---
debugPrintConcatWeights (Codex IDE GPT5Codex prompt):

... please add option debugPrintConcatWeights=True to print (using useLovelyTensors=True);
a) the yHat weights of layersFeedConcatInput, and;
b) the xEmbed weights of layersFeedConcatInput for each batch.

Implementation: When debugPrintConcatWeights=True, complete my pseudo code in the #TODO section of _ActionLayerBase:forward().

---
trainClassificationLayer (Codex IDE GPT5Codex prompt):

Please implement option trainClassificationLayer=True to enable training of classification layer when useClassificationLayerLoss=True (classification layer weights are modified based on backprop gradients).

